{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11a570eb-022e-44fb-80d9-213aa3f10fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "import os,cv2\n",
    "\n",
    "def plot_gallery(images,titles,h,w,n_row=3,n_col=4):\n",
    "    \"\"\"Helper function to plot a gallery of potraits\"\"\"\n",
    "    plt.figure(figsize=(1.8*n_col,2.4*n_row))\n",
    "    plt.subplots_adjust(bottom=0,left=0.1,right=.99,top=.90,hspace=.35)\n",
    "    for i in range(n_row*n_col):\n",
    "        plt.subplot(n_row,n_col,i+1)\n",
    "        plt.imshow(images[i].reshape((h,w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i],size=12)\n",
    "        plt.xticks(())\n",
    "        plt.ytics(())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9c71b64-30fc-4228-a223-7c98e063ae95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450,) (450, 90000) (450,)\n",
      "Number of samples: 450\n",
      "Total dataset size:\n",
      "n_sapmles: 450\n",
      "n_features: 90000\n",
      "n_classes: 450\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dir_name=\"dataset/faces/\"\n",
    "y=[];X=[];target_names=[]\n",
    "person_id=0;h=w=300\n",
    "n_samples=0\n",
    "class_names=[]\n",
    "for person_name in os.listdir(dir_name):\n",
    "    dir_path = dir_name+person_name+\"/\"\n",
    "    class_names.append(person_name)\n",
    "    for image_name in os.listdir(dir_path):\n",
    "        image_path = dir_path+image_name\n",
    "        img = cv2.imread(image_path)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        resized_image = cv2.resize(gray,(h,w))\n",
    "        v=resized_image.flatten()\n",
    "        X.append(v)\n",
    "        n_samples = n_samples+1\n",
    "        y.append(person_id)\n",
    "        target_names.append(person_name)\n",
    "    person_id=person_id+1\n",
    "\n",
    "y=np.array(y)\n",
    "X=np.array(X)\n",
    "target_names = np.array(target_names)\n",
    "n_features = X.shape[1]\n",
    "print(y.shape,X.shape,target_names.shape)\n",
    "print(\"Number of samples:\" ,n_samples)\n",
    "\n",
    "\n",
    "n_classes=target_names.shape[0]\n",
    "\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_sapmles: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f2f765d-0451-437b-bc2a-dc28f022936d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the top 150 eigenfaces from 337 faces\n",
      "Projecting the input data on the eigenfaces orthonormal basis\n",
      "(337, 150) (113, 150)\n",
      "Project done...\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42)\n",
    "n_components = 150\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\" %(n_components, X_train.shape[0]))\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized', whiten=True).fit(X_train)\n",
    "eigenfaces = pca.components_.reshape((n_components,h,w))\n",
    "plt.show()\n",
    "\n",
    "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print(X_train_pca.shape,X_test_pca.shape)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train_pca,y_train)\n",
    "\n",
    "X_train_lda = lda.transform(X_train_pca)\n",
    "X_test_lda = lda.transform(X_test_pca)\n",
    "print(\"Project done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee005adc-79fb-4112-8263-2f435f8d0fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.29552001\n",
      "Iteration 2, loss = 3.23198225\n",
      "Iteration 3, loss = 3.17334704\n",
      "Iteration 4, loss = 3.11463843\n",
      "Iteration 5, loss = 3.05662522\n",
      "Iteration 6, loss = 3.00304068\n",
      "Iteration 7, loss = 2.94982079\n",
      "Iteration 8, loss = 2.89892143\n",
      "Iteration 9, loss = 2.85172009\n",
      "Iteration 10, loss = 2.80475030\n",
      "Iteration 11, loss = 2.75913526\n",
      "Iteration 12, loss = 2.71688270\n",
      "Iteration 13, loss = 2.67450616\n",
      "Iteration 14, loss = 2.63511927\n",
      "Iteration 15, loss = 2.59509734\n",
      "Iteration 16, loss = 2.55728900\n",
      "Iteration 17, loss = 2.52149424\n",
      "Iteration 18, loss = 2.48513431\n",
      "Iteration 19, loss = 2.45267813\n",
      "Iteration 20, loss = 2.41828422\n",
      "Iteration 21, loss = 2.38696067\n",
      "Iteration 22, loss = 2.35462915\n",
      "Iteration 23, loss = 2.32369248\n",
      "Iteration 24, loss = 2.29352728\n",
      "Iteration 25, loss = 2.26404831\n",
      "Iteration 26, loss = 2.23616922\n",
      "Iteration 27, loss = 2.20714214\n",
      "Iteration 28, loss = 2.17969727\n",
      "Iteration 29, loss = 2.15203493\n",
      "Iteration 30, loss = 2.12491157\n",
      "Iteration 31, loss = 2.09875339\n",
      "Iteration 32, loss = 2.07302411\n",
      "Iteration 33, loss = 2.04767306\n",
      "Iteration 34, loss = 2.02184064\n",
      "Iteration 35, loss = 1.99721412\n",
      "Iteration 36, loss = 1.97383700\n",
      "Iteration 37, loss = 1.94964800\n",
      "Iteration 38, loss = 1.92541532\n",
      "Iteration 39, loss = 1.90252068\n",
      "Iteration 40, loss = 1.87920607\n",
      "Iteration 41, loss = 1.85565339\n",
      "Iteration 42, loss = 1.83405385\n",
      "Iteration 43, loss = 1.81196544\n",
      "Iteration 44, loss = 1.79035915\n",
      "Iteration 45, loss = 1.76927751\n",
      "Iteration 46, loss = 1.74820741\n",
      "Iteration 47, loss = 1.72819037\n",
      "Iteration 48, loss = 1.70760858\n",
      "Iteration 49, loss = 1.68783872\n",
      "Iteration 50, loss = 1.66829499\n",
      "Iteration 51, loss = 1.64876924\n",
      "Iteration 52, loss = 1.62999712\n",
      "Iteration 53, loss = 1.61143252\n",
      "Iteration 54, loss = 1.59208326\n",
      "Iteration 55, loss = 1.57385810\n",
      "Iteration 56, loss = 1.55580983\n",
      "Iteration 57, loss = 1.53736639\n",
      "Iteration 58, loss = 1.51944777\n",
      "Iteration 59, loss = 1.50174416\n",
      "Iteration 60, loss = 1.48381451\n",
      "Iteration 61, loss = 1.46600383\n",
      "Iteration 62, loss = 1.44909094\n",
      "Iteration 63, loss = 1.43159211\n",
      "Iteration 64, loss = 1.41405485\n",
      "Iteration 65, loss = 1.39706718\n",
      "Iteration 66, loss = 1.38015303\n",
      "Iteration 67, loss = 1.36297903\n",
      "Iteration 68, loss = 1.34616712\n",
      "Iteration 69, loss = 1.32975050\n",
      "Iteration 70, loss = 1.31359687\n",
      "Iteration 71, loss = 1.29707516\n",
      "Iteration 72, loss = 1.28112713\n",
      "Iteration 73, loss = 1.26555193\n",
      "Iteration 74, loss = 1.24978572\n",
      "Iteration 75, loss = 1.23429365\n",
      "Iteration 76, loss = 1.21898313\n",
      "Iteration 77, loss = 1.20374082\n",
      "Iteration 78, loss = 1.18887821\n",
      "Iteration 79, loss = 1.17436213\n",
      "Iteration 80, loss = 1.15974602\n",
      "Iteration 81, loss = 1.14528710\n",
      "Iteration 82, loss = 1.13135564\n",
      "Iteration 83, loss = 1.11719140\n",
      "Iteration 84, loss = 1.10336048\n",
      "Iteration 85, loss = 1.08963141\n",
      "Iteration 86, loss = 1.07618998\n",
      "Iteration 87, loss = 1.06288890\n",
      "Iteration 88, loss = 1.04966069\n",
      "Iteration 89, loss = 1.03692116\n",
      "Iteration 90, loss = 1.02412519\n",
      "Iteration 91, loss = 1.01146827\n",
      "Iteration 92, loss = 0.99919064\n",
      "Iteration 93, loss = 0.98682583\n",
      "Iteration 94, loss = 0.97503732\n",
      "Iteration 95, loss = 0.96294248\n",
      "Iteration 96, loss = 0.95154058\n",
      "Iteration 97, loss = 0.93981155\n",
      "Iteration 98, loss = 0.92833576\n",
      "Iteration 99, loss = 0.91720561\n",
      "Iteration 100, loss = 0.90618378\n",
      "Iteration 101, loss = 0.89534195\n",
      "Iteration 102, loss = 0.88481408\n",
      "Iteration 103, loss = 0.87425535\n",
      "Iteration 104, loss = 0.86361047\n",
      "Iteration 105, loss = 0.85344644\n",
      "Iteration 106, loss = 0.84306235\n",
      "Iteration 107, loss = 0.83322875\n",
      "Iteration 108, loss = 0.82328099\n",
      "Iteration 109, loss = 0.81379995\n",
      "Iteration 110, loss = 0.80423749\n",
      "Iteration 111, loss = 0.79507702\n",
      "Iteration 112, loss = 0.78577270\n",
      "Iteration 113, loss = 0.77654462\n",
      "Iteration 114, loss = 0.76802628\n",
      "Iteration 115, loss = 0.75908657\n",
      "Iteration 116, loss = 0.75057038\n",
      "Iteration 117, loss = 0.74218040\n",
      "Iteration 118, loss = 0.73404351\n",
      "Iteration 119, loss = 0.72589868\n",
      "Iteration 120, loss = 0.71799914\n",
      "Iteration 121, loss = 0.71027170\n",
      "Iteration 122, loss = 0.70254705\n",
      "Iteration 123, loss = 0.69504298\n",
      "Iteration 124, loss = 0.68765676\n",
      "Iteration 125, loss = 0.68050908\n",
      "Iteration 126, loss = 0.67331798\n",
      "Iteration 127, loss = 0.66636449\n",
      "Iteration 128, loss = 0.65943806\n",
      "Iteration 129, loss = 0.65294597\n",
      "Iteration 130, loss = 0.64634367\n",
      "Iteration 131, loss = 0.63991964\n",
      "Iteration 132, loss = 0.63352741\n",
      "Iteration 133, loss = 0.62741705\n",
      "Iteration 134, loss = 0.62136454\n",
      "Iteration 135, loss = 0.61541213\n",
      "Iteration 136, loss = 0.60972149\n",
      "Iteration 137, loss = 0.60386873\n",
      "Iteration 138, loss = 0.59834253\n",
      "Iteration 139, loss = 0.59286710\n",
      "Iteration 140, loss = 0.58757313\n",
      "Iteration 141, loss = 0.58218409\n",
      "Iteration 142, loss = 0.57692731\n",
      "Iteration 143, loss = 0.57161155\n",
      "Iteration 144, loss = 0.56662252\n",
      "Iteration 145, loss = 0.56157996\n",
      "Iteration 146, loss = 0.55664036\n",
      "Iteration 147, loss = 0.55161384\n",
      "Iteration 148, loss = 0.54682246\n",
      "Iteration 149, loss = 0.54216416\n",
      "Iteration 150, loss = 0.53741774\n",
      "Iteration 151, loss = 0.53277650\n",
      "Iteration 152, loss = 0.52825864\n",
      "Iteration 153, loss = 0.52380618\n",
      "Iteration 154, loss = 0.51938122\n",
      "Iteration 155, loss = 0.51505089\n",
      "Iteration 156, loss = 0.51066436\n",
      "Iteration 157, loss = 0.50635829\n",
      "Iteration 158, loss = 0.50214553\n",
      "Iteration 159, loss = 0.49802611\n",
      "Iteration 160, loss = 0.49376036\n",
      "Iteration 161, loss = 0.48966514\n",
      "Iteration 162, loss = 0.48545106\n",
      "Iteration 163, loss = 0.48106589\n",
      "Iteration 164, loss = 0.47682746\n",
      "Iteration 165, loss = 0.47231320\n",
      "Iteration 166, loss = 0.46769595\n",
      "Iteration 167, loss = 0.46301536\n",
      "Iteration 168, loss = 0.45848161\n",
      "Iteration 169, loss = 0.45364363\n",
      "Iteration 170, loss = 0.44900095\n",
      "Iteration 171, loss = 0.44439137\n",
      "Iteration 172, loss = 0.43961301\n",
      "Iteration 173, loss = 0.43503348\n",
      "Iteration 174, loss = 0.43027845\n",
      "Iteration 175, loss = 0.42552476\n",
      "Iteration 176, loss = 0.42091690\n",
      "Iteration 177, loss = 0.41611000\n",
      "Iteration 178, loss = 0.41159897\n",
      "Iteration 179, loss = 0.40715794\n",
      "Iteration 180, loss = 0.40218806\n",
      "Iteration 181, loss = 0.39767139\n",
      "Iteration 182, loss = 0.39290110\n",
      "Iteration 183, loss = 0.38814310\n",
      "Iteration 184, loss = 0.38392325\n",
      "Iteration 185, loss = 0.37909181\n",
      "Iteration 186, loss = 0.37435812\n",
      "Iteration 187, loss = 0.36969154\n",
      "Iteration 188, loss = 0.36511114\n",
      "Iteration 189, loss = 0.36065429\n",
      "Iteration 190, loss = 0.35616781\n",
      "Iteration 191, loss = 0.35153363\n",
      "Iteration 192, loss = 0.34697805\n",
      "Iteration 193, loss = 0.34258999\n",
      "Iteration 194, loss = 0.33794036\n",
      "Iteration 195, loss = 0.33359259\n",
      "Iteration 196, loss = 0.32908995\n",
      "Iteration 197, loss = 0.32457048\n",
      "Iteration 198, loss = 0.32052906\n",
      "Iteration 199, loss = 0.31635995\n",
      "Iteration 200, loss = 0.31210543\n",
      "Iteration 201, loss = 0.30789256\n",
      "Iteration 202, loss = 0.30390683\n",
      "Iteration 203, loss = 0.30001744\n",
      "Iteration 204, loss = 0.29611641\n",
      "Iteration 205, loss = 0.29249511\n",
      "Iteration 206, loss = 0.28854061\n",
      "Iteration 207, loss = 0.28503211\n",
      "Iteration 208, loss = 0.28124548\n",
      "Iteration 209, loss = 0.27766711\n",
      "Iteration 210, loss = 0.27418778\n",
      "Iteration 211, loss = 0.27075475\n",
      "Iteration 212, loss = 0.26736352\n",
      "Iteration 213, loss = 0.26430229\n",
      "Iteration 214, loss = 0.26063242\n",
      "Iteration 215, loss = 0.25777523\n",
      "Iteration 216, loss = 0.25458621\n",
      "Iteration 217, loss = 0.25147840\n",
      "Iteration 218, loss = 0.24863338\n",
      "Iteration 219, loss = 0.24564125\n",
      "Iteration 220, loss = 0.24315096\n",
      "Iteration 221, loss = 0.24026833\n",
      "Iteration 222, loss = 0.23753044\n",
      "Iteration 223, loss = 0.23494437\n",
      "Iteration 224, loss = 0.23228338\n",
      "Iteration 225, loss = 0.22978773\n",
      "Iteration 226, loss = 0.22724094\n",
      "Iteration 227, loss = 0.22491794\n",
      "Iteration 228, loss = 0.22242868\n",
      "Iteration 229, loss = 0.22006858\n",
      "Iteration 230, loss = 0.21762426\n",
      "Iteration 231, loss = 0.21537127\n",
      "Iteration 232, loss = 0.21307533\n",
      "Iteration 233, loss = 0.21090437\n",
      "Iteration 234, loss = 0.20878987\n",
      "Iteration 235, loss = 0.20667280\n",
      "Iteration 236, loss = 0.20458939\n",
      "Iteration 237, loss = 0.20260177\n",
      "Iteration 238, loss = 0.20064021\n",
      "Iteration 239, loss = 0.19868066\n",
      "Iteration 240, loss = 0.19681344\n",
      "Iteration 241, loss = 0.19505799\n",
      "Iteration 242, loss = 0.19316962\n",
      "Iteration 243, loss = 0.19150245\n",
      "Iteration 244, loss = 0.18979391\n",
      "Iteration 245, loss = 0.18817604\n",
      "Iteration 246, loss = 0.18655962\n",
      "Iteration 247, loss = 0.18490989\n",
      "Iteration 248, loss = 0.18340900\n",
      "Iteration 249, loss = 0.18183014\n",
      "Iteration 250, loss = 0.18037670\n",
      "Iteration 251, loss = 0.17894969\n",
      "Iteration 252, loss = 0.17751739\n",
      "Iteration 253, loss = 0.17611126\n",
      "Iteration 254, loss = 0.17472708\n",
      "Iteration 255, loss = 0.17334888\n",
      "Iteration 256, loss = 0.17190238\n",
      "Iteration 257, loss = 0.17050352\n",
      "Iteration 258, loss = 0.16918269\n",
      "Iteration 259, loss = 0.16784385\n",
      "Iteration 260, loss = 0.16647617\n",
      "Iteration 261, loss = 0.16511375\n",
      "Iteration 262, loss = 0.16384793\n",
      "Iteration 263, loss = 0.16263016\n",
      "Iteration 264, loss = 0.16127600\n",
      "Iteration 265, loss = 0.16010273\n",
      "Iteration 266, loss = 0.15890544\n",
      "Iteration 267, loss = 0.15769635\n",
      "Iteration 268, loss = 0.15651886\n",
      "Iteration 269, loss = 0.15532699\n",
      "Iteration 270, loss = 0.15422861\n",
      "Iteration 271, loss = 0.15307981\n",
      "Iteration 272, loss = 0.15201191\n",
      "Iteration 273, loss = 0.15088146\n",
      "Iteration 274, loss = 0.14981766\n",
      "Iteration 275, loss = 0.14880029\n",
      "Iteration 276, loss = 0.14768362\n",
      "Iteration 277, loss = 0.14670981\n",
      "Iteration 278, loss = 0.14564320\n",
      "Iteration 279, loss = 0.14461860\n",
      "Iteration 280, loss = 0.14364470\n",
      "Iteration 281, loss = 0.14271576\n",
      "Iteration 282, loss = 0.14172554\n",
      "Iteration 283, loss = 0.14076194\n",
      "Iteration 284, loss = 0.13984433\n",
      "Iteration 285, loss = 0.13891488\n",
      "Iteration 286, loss = 0.13799320\n",
      "Iteration 287, loss = 0.13712350\n",
      "Iteration 288, loss = 0.13619877\n",
      "Iteration 289, loss = 0.13538663\n",
      "Iteration 290, loss = 0.13450222\n",
      "Iteration 291, loss = 0.13365682\n",
      "Iteration 292, loss = 0.13290249\n",
      "Iteration 293, loss = 0.13204501\n",
      "Iteration 294, loss = 0.13129404\n",
      "Iteration 295, loss = 0.13045134\n",
      "Iteration 296, loss = 0.12970723\n",
      "Iteration 297, loss = 0.12897104\n",
      "Iteration 298, loss = 0.12817716\n",
      "Iteration 299, loss = 0.12746347\n",
      "Iteration 300, loss = 0.12668277\n",
      "Iteration 301, loss = 0.12597914\n",
      "Iteration 302, loss = 0.12526728\n",
      "Iteration 303, loss = 0.12455347\n",
      "Iteration 304, loss = 0.12384751\n",
      "Iteration 305, loss = 0.12316013\n",
      "Iteration 306, loss = 0.12246618\n",
      "Iteration 307, loss = 0.12184159\n",
      "Iteration 308, loss = 0.12115202\n",
      "Iteration 309, loss = 0.12045089\n",
      "Iteration 310, loss = 0.11978914\n",
      "Iteration 311, loss = 0.11916536\n",
      "Iteration 312, loss = 0.11846405\n",
      "Iteration 313, loss = 0.11781707\n",
      "Iteration 314, loss = 0.11713619\n",
      "Iteration 315, loss = 0.11650255\n",
      "Iteration 316, loss = 0.11587696\n",
      "Iteration 317, loss = 0.11528956\n",
      "Iteration 318, loss = 0.11459848\n",
      "Iteration 319, loss = 0.11396562\n",
      "Iteration 320, loss = 0.11336608\n",
      "Iteration 321, loss = 0.11271220\n",
      "Iteration 322, loss = 0.11212752\n",
      "Iteration 323, loss = 0.11152182\n",
      "Iteration 324, loss = 0.11091858\n",
      "Iteration 325, loss = 0.11031959\n",
      "Iteration 326, loss = 0.10974014\n",
      "Iteration 327, loss = 0.10917677\n",
      "Iteration 328, loss = 0.10858365\n",
      "Iteration 329, loss = 0.10799814\n",
      "Iteration 330, loss = 0.10747837\n",
      "Iteration 331, loss = 0.10688716\n",
      "Iteration 332, loss = 0.10635007\n",
      "Iteration 333, loss = 0.10577552\n",
      "Iteration 334, loss = 0.10527477\n",
      "Iteration 335, loss = 0.10469414\n",
      "Iteration 336, loss = 0.10418326\n",
      "Iteration 337, loss = 0.10363959\n",
      "Iteration 338, loss = 0.10306931\n",
      "Iteration 339, loss = 0.10259356\n",
      "Iteration 340, loss = 0.10205767\n",
      "Iteration 341, loss = 0.10156520\n",
      "Iteration 342, loss = 0.10099492\n",
      "Iteration 343, loss = 0.10045341\n",
      "Iteration 344, loss = 0.09986037\n",
      "Iteration 345, loss = 0.09935763\n",
      "Iteration 346, loss = 0.09878940\n",
      "Iteration 347, loss = 0.09815126\n",
      "Iteration 348, loss = 0.09765473\n",
      "Iteration 349, loss = 0.09704475\n",
      "Iteration 350, loss = 0.09647851\n",
      "Iteration 351, loss = 0.09587265\n",
      "Iteration 352, loss = 0.09531964\n",
      "Iteration 353, loss = 0.09481024\n",
      "Iteration 354, loss = 0.09420406\n",
      "Iteration 355, loss = 0.09366276\n",
      "Iteration 356, loss = 0.09312636\n",
      "Iteration 357, loss = 0.09260934\n",
      "Iteration 358, loss = 0.09201380\n",
      "Iteration 359, loss = 0.09155534\n",
      "Iteration 360, loss = 0.09096919\n",
      "Iteration 361, loss = 0.09048346\n",
      "Iteration 362, loss = 0.08997618\n",
      "Iteration 363, loss = 0.08948109\n",
      "Iteration 364, loss = 0.08895055\n",
      "Iteration 365, loss = 0.08848284\n",
      "Iteration 366, loss = 0.08798289\n",
      "Iteration 367, loss = 0.08751128\n",
      "Iteration 368, loss = 0.08705050\n",
      "Iteration 369, loss = 0.08658907\n",
      "Iteration 370, loss = 0.08613458\n",
      "Iteration 371, loss = 0.08570359\n",
      "Iteration 372, loss = 0.08530594\n",
      "Iteration 373, loss = 0.08489246\n",
      "Iteration 374, loss = 0.08443544\n",
      "Iteration 375, loss = 0.08404862\n",
      "Iteration 376, loss = 0.08365308\n",
      "Iteration 377, loss = 0.08324576\n",
      "Iteration 378, loss = 0.08285711\n",
      "Iteration 379, loss = 0.08244146\n",
      "Iteration 380, loss = 0.08202679\n",
      "Iteration 381, loss = 0.08162775\n",
      "Iteration 382, loss = 0.08125778\n",
      "Iteration 383, loss = 0.08083562\n",
      "Iteration 384, loss = 0.08047211\n",
      "Iteration 385, loss = 0.08009133\n",
      "Iteration 386, loss = 0.07969861\n",
      "Iteration 387, loss = 0.07931920\n",
      "Iteration 388, loss = 0.07892612\n",
      "Iteration 389, loss = 0.07855325\n",
      "Iteration 390, loss = 0.07821315\n",
      "Iteration 391, loss = 0.07783796\n",
      "Iteration 392, loss = 0.07746131\n",
      "Iteration 393, loss = 0.07708371\n",
      "Iteration 394, loss = 0.07675124\n",
      "Iteration 395, loss = 0.07639681\n",
      "Iteration 396, loss = 0.07604355\n",
      "Iteration 397, loss = 0.07571044\n",
      "Iteration 398, loss = 0.07533248\n",
      "Iteration 399, loss = 0.07500742\n",
      "Iteration 400, loss = 0.07466195\n",
      "Iteration 401, loss = 0.07433824\n",
      "Iteration 402, loss = 0.07400364\n",
      "Iteration 403, loss = 0.07365632\n",
      "Iteration 404, loss = 0.07332770\n",
      "Iteration 405, loss = 0.07298769\n",
      "Iteration 406, loss = 0.07267375\n",
      "Iteration 407, loss = 0.07235286\n",
      "Iteration 408, loss = 0.07202685\n",
      "Iteration 409, loss = 0.07171820\n",
      "Iteration 410, loss = 0.07138979\n",
      "Iteration 411, loss = 0.07107329\n",
      "Iteration 412, loss = 0.07076242\n",
      "Iteration 413, loss = 0.07045512\n",
      "Iteration 414, loss = 0.07014865\n",
      "Iteration 415, loss = 0.06984031\n",
      "Iteration 416, loss = 0.06954552\n",
      "Iteration 417, loss = 0.06922766\n",
      "Iteration 418, loss = 0.06893942\n",
      "Iteration 419, loss = 0.06863912\n",
      "Iteration 420, loss = 0.06833509\n",
      "Iteration 421, loss = 0.06804770\n",
      "Iteration 422, loss = 0.06777929\n",
      "Iteration 423, loss = 0.06748326\n",
      "Iteration 424, loss = 0.06718903\n",
      "Iteration 425, loss = 0.06689853\n",
      "Iteration 426, loss = 0.06660741\n",
      "Iteration 427, loss = 0.06631524\n",
      "Iteration 428, loss = 0.06606246\n",
      "Iteration 429, loss = 0.06575317\n",
      "Iteration 430, loss = 0.06550924\n",
      "Iteration 431, loss = 0.06519374\n",
      "Iteration 432, loss = 0.06493588\n",
      "Iteration 433, loss = 0.06467400\n",
      "Iteration 434, loss = 0.06438951\n",
      "Iteration 435, loss = 0.06411917\n",
      "Iteration 436, loss = 0.06385216\n",
      "Iteration 437, loss = 0.06355699\n",
      "Iteration 438, loss = 0.06332860\n",
      "Iteration 439, loss = 0.06307108\n",
      "Iteration 440, loss = 0.06277204\n",
      "Iteration 441, loss = 0.06254246\n",
      "Iteration 442, loss = 0.06225560\n",
      "Iteration 443, loss = 0.06202196\n",
      "Iteration 444, loss = 0.06176839\n",
      "Iteration 445, loss = 0.06152644\n",
      "Iteration 446, loss = 0.06124347\n",
      "Iteration 447, loss = 0.06101519\n",
      "Iteration 448, loss = 0.06077142\n",
      "Iteration 449, loss = 0.06050617\n",
      "Iteration 450, loss = 0.06025467\n",
      "Iteration 451, loss = 0.06001847\n",
      "Iteration 452, loss = 0.05978228\n",
      "Iteration 453, loss = 0.05953096\n",
      "Iteration 454, loss = 0.05930267\n",
      "Iteration 455, loss = 0.05907096\n",
      "Iteration 456, loss = 0.05884505\n",
      "Iteration 457, loss = 0.05857450\n",
      "Iteration 458, loss = 0.05836833\n",
      "Iteration 459, loss = 0.05811542\n",
      "Iteration 460, loss = 0.05787403\n",
      "Iteration 461, loss = 0.05764446\n",
      "Iteration 462, loss = 0.05741354\n",
      "Iteration 463, loss = 0.05718931\n",
      "Iteration 464, loss = 0.05696730\n",
      "Iteration 465, loss = 0.05673512\n",
      "Iteration 466, loss = 0.05649749\n",
      "Iteration 467, loss = 0.05628461\n",
      "Iteration 468, loss = 0.05606713\n",
      "Iteration 469, loss = 0.05581923\n",
      "Iteration 470, loss = 0.05559956\n",
      "Iteration 471, loss = 0.05539359\n",
      "Iteration 472, loss = 0.05517573\n",
      "Iteration 473, loss = 0.05495657\n",
      "Iteration 474, loss = 0.05476044\n",
      "Iteration 475, loss = 0.05451591\n",
      "Iteration 476, loss = 0.05432064\n",
      "Iteration 477, loss = 0.05410402\n",
      "Iteration 478, loss = 0.05389142\n",
      "Iteration 479, loss = 0.05367033\n",
      "Iteration 480, loss = 0.05345896\n",
      "Iteration 481, loss = 0.05325084\n",
      "Iteration 482, loss = 0.05305148\n",
      "Iteration 483, loss = 0.05283688\n",
      "Iteration 484, loss = 0.05264057\n",
      "Iteration 485, loss = 0.05243180\n",
      "Iteration 486, loss = 0.05223235\n",
      "Iteration 487, loss = 0.05204225\n",
      "Iteration 488, loss = 0.05183334\n",
      "Iteration 489, loss = 0.05161572\n",
      "Iteration 490, loss = 0.05143427\n",
      "Iteration 491, loss = 0.05123747\n",
      "Iteration 492, loss = 0.05103374\n",
      "Iteration 493, loss = 0.05085382\n",
      "Iteration 494, loss = 0.05065412\n",
      "Iteration 495, loss = 0.05046446\n",
      "Iteration 496, loss = 0.05025828\n",
      "Iteration 497, loss = 0.05008273\n",
      "Iteration 498, loss = 0.04989888\n",
      "Iteration 499, loss = 0.04969898\n",
      "Iteration 500, loss = 0.04952352\n",
      "Iteration 501, loss = 0.04933988\n",
      "Iteration 502, loss = 0.04915452\n",
      "Iteration 503, loss = 0.04896420\n",
      "Iteration 504, loss = 0.04879240\n",
      "Iteration 505, loss = 0.04860278\n",
      "Iteration 506, loss = 0.04844195\n",
      "Iteration 507, loss = 0.04825041\n",
      "Iteration 508, loss = 0.04807899\n",
      "Iteration 509, loss = 0.04791547\n",
      "Iteration 510, loss = 0.04773886\n",
      "Iteration 511, loss = 0.04756363\n",
      "Iteration 512, loss = 0.04739853\n",
      "Iteration 513, loss = 0.04722521\n",
      "Iteration 514, loss = 0.04706065\n",
      "Iteration 515, loss = 0.04689192\n",
      "Iteration 516, loss = 0.04673123\n",
      "Iteration 517, loss = 0.04655134\n",
      "Iteration 518, loss = 0.04639206\n",
      "Iteration 519, loss = 0.04622812\n",
      "Iteration 520, loss = 0.04605721\n",
      "Iteration 521, loss = 0.04588413\n",
      "Iteration 522, loss = 0.04572916\n",
      "Iteration 523, loss = 0.04557487\n",
      "Iteration 524, loss = 0.04540136\n",
      "Iteration 525, loss = 0.04524487\n",
      "Iteration 526, loss = 0.04509410\n",
      "Iteration 527, loss = 0.04491271\n",
      "Iteration 528, loss = 0.04476307\n",
      "Iteration 529, loss = 0.04459330\n",
      "Iteration 530, loss = 0.04443889\n",
      "Iteration 531, loss = 0.04428652\n",
      "Iteration 532, loss = 0.04411880\n",
      "Iteration 533, loss = 0.04397800\n",
      "Iteration 534, loss = 0.04381207\n",
      "Iteration 535, loss = 0.04365187\n",
      "Iteration 536, loss = 0.04349975\n",
      "Iteration 537, loss = 0.04334490\n",
      "Iteration 538, loss = 0.04319729\n",
      "Iteration 539, loss = 0.04305801\n",
      "Iteration 540, loss = 0.04290137\n",
      "Iteration 541, loss = 0.04274764\n",
      "Iteration 542, loss = 0.04260199\n",
      "Iteration 543, loss = 0.04244283\n",
      "Iteration 544, loss = 0.04230500\n",
      "Iteration 545, loss = 0.04215101\n",
      "Iteration 546, loss = 0.04201017\n",
      "Iteration 547, loss = 0.04185364\n",
      "Iteration 548, loss = 0.04171272\n",
      "Iteration 549, loss = 0.04157820\n",
      "Iteration 550, loss = 0.04142697\n",
      "Iteration 551, loss = 0.04128382\n",
      "Iteration 552, loss = 0.04113441\n",
      "Iteration 553, loss = 0.04099583\n",
      "Iteration 554, loss = 0.04085579\n",
      "Iteration 555, loss = 0.04072407\n",
      "Iteration 556, loss = 0.04057376\n",
      "Iteration 557, loss = 0.04044060\n",
      "Iteration 558, loss = 0.04029214\n",
      "Iteration 559, loss = 0.04015971\n",
      "Iteration 560, loss = 0.04001564\n",
      "Iteration 561, loss = 0.03988981\n",
      "Iteration 562, loss = 0.03974007\n",
      "Iteration 563, loss = 0.03962411\n",
      "Iteration 564, loss = 0.03948045\n",
      "Iteration 565, loss = 0.03933720\n",
      "Iteration 566, loss = 0.03920866\n",
      "Iteration 567, loss = 0.03907819\n",
      "Iteration 568, loss = 0.03894165\n",
      "Iteration 569, loss = 0.03880362\n",
      "Iteration 570, loss = 0.03867219\n",
      "Iteration 571, loss = 0.03853695\n",
      "Iteration 572, loss = 0.03840944\n",
      "Iteration 573, loss = 0.03829111\n",
      "Iteration 574, loss = 0.03814613\n",
      "Iteration 575, loss = 0.03802437\n",
      "Iteration 576, loss = 0.03789025\n",
      "Iteration 577, loss = 0.03776494\n",
      "Iteration 578, loss = 0.03764190\n",
      "Iteration 579, loss = 0.03751409\n",
      "Iteration 580, loss = 0.03740461\n",
      "Iteration 581, loss = 0.03725686\n",
      "Iteration 582, loss = 0.03715391\n",
      "Iteration 583, loss = 0.03702007\n",
      "Iteration 584, loss = 0.03688657\n",
      "Iteration 585, loss = 0.03676567\n",
      "Iteration 586, loss = 0.03664749\n",
      "Iteration 587, loss = 0.03652204\n",
      "Iteration 588, loss = 0.03639474\n",
      "Iteration 589, loss = 0.03628573\n",
      "Iteration 590, loss = 0.03615196\n",
      "Iteration 591, loss = 0.03604202\n",
      "Iteration 592, loss = 0.03591061\n",
      "Iteration 593, loss = 0.03579091\n",
      "Iteration 594, loss = 0.03567772\n",
      "Iteration 595, loss = 0.03555221\n",
      "Iteration 596, loss = 0.03544118\n",
      "Iteration 597, loss = 0.03530552\n",
      "Iteration 598, loss = 0.03519971\n",
      "Iteration 599, loss = 0.03507834\n",
      "Iteration 600, loss = 0.03495914\n",
      "Iteration 601, loss = 0.03483602\n",
      "Iteration 602, loss = 0.03473644\n",
      "Iteration 603, loss = 0.03461727\n",
      "Iteration 604, loss = 0.03450582\n",
      "Iteration 605, loss = 0.03438406\n",
      "Iteration 606, loss = 0.03426341\n",
      "Iteration 607, loss = 0.03415169\n",
      "Iteration 608, loss = 0.03404490\n",
      "Iteration 609, loss = 0.03391550\n",
      "Iteration 610, loss = 0.03380985\n",
      "Iteration 611, loss = 0.03370069\n",
      "Iteration 612, loss = 0.03358599\n",
      "Iteration 613, loss = 0.03347066\n",
      "Iteration 614, loss = 0.03336220\n",
      "Iteration 615, loss = 0.03326329\n",
      "Iteration 616, loss = 0.03314051\n",
      "Iteration 617, loss = 0.03303352\n",
      "Iteration 618, loss = 0.03292932\n",
      "Iteration 619, loss = 0.03281020\n",
      "Iteration 620, loss = 0.03270303\n",
      "Iteration 621, loss = 0.03258891\n",
      "Iteration 622, loss = 0.03249075\n",
      "Iteration 623, loss = 0.03238743\n",
      "Iteration 624, loss = 0.03228699\n",
      "Iteration 625, loss = 0.03217567\n",
      "Iteration 626, loss = 0.03207049\n",
      "Iteration 627, loss = 0.03197633\n",
      "Iteration 628, loss = 0.03188224\n",
      "Iteration 629, loss = 0.03177932\n",
      "Iteration 630, loss = 0.03168268\n",
      "Iteration 631, loss = 0.03157552\n",
      "Iteration 632, loss = 0.03146800\n",
      "Iteration 633, loss = 0.03137121\n",
      "Iteration 634, loss = 0.03126769\n",
      "Iteration 635, loss = 0.03116488\n",
      "Iteration 636, loss = 0.03106404\n",
      "Iteration 637, loss = 0.03097852\n",
      "Iteration 638, loss = 0.03087989\n",
      "Iteration 639, loss = 0.03077995\n",
      "Iteration 640, loss = 0.03067693\n",
      "Iteration 641, loss = 0.03058197\n",
      "Iteration 642, loss = 0.03048211\n",
      "Iteration 643, loss = 0.03038088\n",
      "Iteration 644, loss = 0.03028627\n",
      "Iteration 645, loss = 0.03020438\n",
      "Iteration 646, loss = 0.03009491\n",
      "Iteration 647, loss = 0.03000931\n",
      "Iteration 648, loss = 0.02991101\n",
      "Iteration 649, loss = 0.02982233\n",
      "Iteration 650, loss = 0.02973244\n",
      "Iteration 651, loss = 0.02964086\n",
      "Iteration 652, loss = 0.02954184\n",
      "Iteration 653, loss = 0.02945576\n",
      "Iteration 654, loss = 0.02936599\n",
      "Iteration 655, loss = 0.02926557\n",
      "Iteration 656, loss = 0.02918527\n",
      "Iteration 657, loss = 0.02909972\n",
      "Iteration 658, loss = 0.02901092\n",
      "Iteration 659, loss = 0.02892530\n",
      "Iteration 660, loss = 0.02884129\n",
      "Iteration 661, loss = 0.02874029\n",
      "Iteration 662, loss = 0.02865108\n",
      "Iteration 663, loss = 0.02857623\n",
      "Iteration 664, loss = 0.02848450\n",
      "Iteration 665, loss = 0.02840097\n",
      "Iteration 666, loss = 0.02831219\n",
      "Iteration 667, loss = 0.02822413\n",
      "Iteration 668, loss = 0.02813895\n",
      "Iteration 669, loss = 0.02805663\n",
      "Iteration 670, loss = 0.02797107\n",
      "Iteration 671, loss = 0.02788590\n",
      "Iteration 672, loss = 0.02780643\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Model Weights:\n",
      "[(8, 10), (10, 10), (10, 9)]\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(random_state=1, hidden_layer_sizes=(10,10),max_iter=1000, verbose=True).fit(X_train_lda, y_train)\n",
    "print(\"Model Weights:\")\n",
    "model_info = [coef.shape for coef in clf.coefs_]\n",
    "print(model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66feb398-2cfe-4380-9f37-36b89069caae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
