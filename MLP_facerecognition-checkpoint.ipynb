{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11a570eb-022e-44fb-80d9-213aa3f10fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "import os,cv2\n",
    "\n",
    "def plot_gallery(images,titles,h,w,n_row=3,n_col=4):\n",
    "    \"\"\"Helper function to plot a gallery of potraits\"\"\"\n",
    "    plt.figure(figsize=(1.8*n_col,2.4*n_row))\n",
    "    plt.subplots_adjust(bottom=0,left=0.1,right=.99,top=.90,hspace=.35)\n",
    "    for i in range(n_row*n_col):\n",
    "        plt.subplot(n_row,n_col,i+1)\n",
    "        plt.imshow(images[i].reshape((h,w)), cmap=plt.cm.gray)\n",
    "        plt.title(titles[i],size=12)\n",
    "        plt.xticks(())\n",
    "        plt.ytics(())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9c71b64-30fc-4228-a223-7c98e063ae95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450,) (450, 90000) (450,)\n",
      "Number of samples: 450\n",
      "Total dataset size:\n",
      "n_sapmles: 450\n",
      "n_features: 90000\n",
      "n_classes: 450\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dir_name=\"dataset/faces/\"\n",
    "y=[];X=[];target_names=[]\n",
    "person_id=0;h=w=300\n",
    "n_samples=0\n",
    "class_names=[]\n",
    "for person_name in os.listdir(dir_name):\n",
    "    dir_path = dir_name+person_name+\"/\"\n",
    "    class_names.append(person_name)\n",
    "    for image_name in os.listdir(dir_path):\n",
    "        image_path = dir_path+image_name\n",
    "        img = cv2.imread(image_path)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        resized_image = cv2.resize(gray,(h,w))\n",
    "        v=resized_image.flatten()\n",
    "        X.append(v)\n",
    "        n_samples = n_samples+1\n",
    "        y.append(person_id)\n",
    "        target_names.append(person_name)\n",
    "    person_id=person_id+1\n",
    "\n",
    "y=np.array(y)\n",
    "X=np.array(X)\n",
    "target_names = np.array(target_names)\n",
    "n_features = X.shape[1]\n",
    "print(y.shape,X.shape,target_names.shape)\n",
    "print(\"Number of samples:\" ,n_samples)\n",
    "\n",
    "\n",
    "n_classes=target_names.shape[0]\n",
    "\n",
    "print(\"Total dataset size:\")\n",
    "print(\"n_sapmles: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f2f765d-0451-437b-bc2a-dc28f022936d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting the top 150 eigenfaces from 337 faces\n",
      "Projecting the input data on the eigenfaces orthonormal basis\n",
      "(337, 150) (113, 150)\n",
      "Project done...\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42)\n",
    "n_components = 150\n",
    "print(\"Extracting the top %d eigenfaces from %d faces\" %(n_components, X_train.shape[0]))\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized', whiten=True).fit(X_train)\n",
    "eigenfaces = pca.components_.reshape((n_components,h,w))\n",
    "plt.show()\n",
    "\n",
    "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print(X_train_pca.shape,X_test_pca.shape)\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train_pca,y_train)\n",
    "\n",
    "X_train_lda = lda.transform(X_train_pca)\n",
    "X_test_lda = lda.transform(X_test_pca)\n",
    "print(\"Project done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee005adc-79fb-4112-8263-2f435f8d0fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.99921146\n",
      "Iteration 2, loss = 2.94030416\n",
      "Iteration 3, loss = 2.88678525\n",
      "Iteration 4, loss = 2.83200395\n",
      "Iteration 5, loss = 2.77804631\n",
      "Iteration 6, loss = 2.72813153\n",
      "Iteration 7, loss = 2.67852013\n",
      "Iteration 8, loss = 2.63106885\n",
      "Iteration 9, loss = 2.58629274\n",
      "Iteration 10, loss = 2.54376859\n",
      "Iteration 11, loss = 2.50158374\n",
      "Iteration 12, loss = 2.46285093\n",
      "Iteration 13, loss = 2.42504234\n",
      "Iteration 14, loss = 2.39016655\n",
      "Iteration 15, loss = 2.35433526\n",
      "Iteration 16, loss = 2.32118070\n",
      "Iteration 17, loss = 2.28875919\n",
      "Iteration 18, loss = 2.25648370\n",
      "Iteration 19, loss = 2.22853311\n",
      "Iteration 20, loss = 2.19831969\n",
      "Iteration 21, loss = 2.17124394\n",
      "Iteration 22, loss = 2.14370697\n",
      "Iteration 23, loss = 2.11698982\n",
      "Iteration 24, loss = 2.09141410\n",
      "Iteration 25, loss = 2.06708066\n",
      "Iteration 26, loss = 2.04356394\n",
      "Iteration 27, loss = 2.02048369\n",
      "Iteration 28, loss = 1.99795526\n",
      "Iteration 29, loss = 1.97684723\n",
      "Iteration 30, loss = 1.95599179\n",
      "Iteration 31, loss = 1.93639821\n",
      "Iteration 32, loss = 1.91740233\n",
      "Iteration 33, loss = 1.89897364\n",
      "Iteration 34, loss = 1.88117958\n",
      "Iteration 35, loss = 1.86368461\n",
      "Iteration 36, loss = 1.84691693\n",
      "Iteration 37, loss = 1.83047669\n",
      "Iteration 38, loss = 1.81434109\n",
      "Iteration 39, loss = 1.79876889\n",
      "Iteration 40, loss = 1.78344696\n",
      "Iteration 41, loss = 1.76829937\n",
      "Iteration 42, loss = 1.75379744\n",
      "Iteration 43, loss = 1.73964905\n",
      "Iteration 44, loss = 1.72555858\n",
      "Iteration 45, loss = 1.71211598\n",
      "Iteration 46, loss = 1.69880519\n",
      "Iteration 47, loss = 1.68606647\n",
      "Iteration 48, loss = 1.67277342\n",
      "Iteration 49, loss = 1.66036141\n",
      "Iteration 50, loss = 1.64755138\n",
      "Iteration 51, loss = 1.63519268\n",
      "Iteration 52, loss = 1.62255027\n",
      "Iteration 53, loss = 1.61046495\n",
      "Iteration 54, loss = 1.59809868\n",
      "Iteration 55, loss = 1.58568656\n",
      "Iteration 56, loss = 1.57359253\n",
      "Iteration 57, loss = 1.56125646\n",
      "Iteration 58, loss = 1.54873917\n",
      "Iteration 59, loss = 1.53692941\n",
      "Iteration 60, loss = 1.52437364\n",
      "Iteration 61, loss = 1.51218345\n",
      "Iteration 62, loss = 1.49973488\n",
      "Iteration 63, loss = 1.48769258\n",
      "Iteration 64, loss = 1.47539655\n",
      "Iteration 65, loss = 1.46338843\n",
      "Iteration 66, loss = 1.45136019\n",
      "Iteration 67, loss = 1.43908247\n",
      "Iteration 68, loss = 1.42713240\n",
      "Iteration 69, loss = 1.41454871\n",
      "Iteration 70, loss = 1.40241879\n",
      "Iteration 71, loss = 1.39000579\n",
      "Iteration 72, loss = 1.37741303\n",
      "Iteration 73, loss = 1.36498677\n",
      "Iteration 74, loss = 1.35252615\n",
      "Iteration 75, loss = 1.33999646\n",
      "Iteration 76, loss = 1.32646163\n",
      "Iteration 77, loss = 1.31341745\n",
      "Iteration 78, loss = 1.29973098\n",
      "Iteration 79, loss = 1.28623131\n",
      "Iteration 80, loss = 1.27167530\n",
      "Iteration 81, loss = 1.25805866\n",
      "Iteration 82, loss = 1.24335985\n",
      "Iteration 83, loss = 1.22813634\n",
      "Iteration 84, loss = 1.21346950\n",
      "Iteration 85, loss = 1.19835281\n",
      "Iteration 86, loss = 1.18333123\n",
      "Iteration 87, loss = 1.16813006\n",
      "Iteration 88, loss = 1.15342623\n",
      "Iteration 89, loss = 1.13846993\n",
      "Iteration 90, loss = 1.12420508\n",
      "Iteration 91, loss = 1.10992196\n",
      "Iteration 92, loss = 1.09645671\n",
      "Iteration 93, loss = 1.08314218\n",
      "Iteration 94, loss = 1.06943455\n",
      "Iteration 95, loss = 1.05671684\n",
      "Iteration 96, loss = 1.04398366\n",
      "Iteration 97, loss = 1.03092775\n",
      "Iteration 98, loss = 1.01866123\n",
      "Iteration 99, loss = 1.00625231\n",
      "Iteration 100, loss = 0.99404997\n",
      "Iteration 101, loss = 0.98169346\n",
      "Iteration 102, loss = 0.96941149\n",
      "Iteration 103, loss = 0.95734938\n",
      "Iteration 104, loss = 0.94484161\n",
      "Iteration 105, loss = 0.93293031\n",
      "Iteration 106, loss = 0.92049862\n",
      "Iteration 107, loss = 0.90830225\n",
      "Iteration 108, loss = 0.89589993\n",
      "Iteration 109, loss = 0.88301415\n",
      "Iteration 110, loss = 0.86913828\n",
      "Iteration 111, loss = 0.85470619\n",
      "Iteration 112, loss = 0.84004582\n",
      "Iteration 113, loss = 0.82538422\n",
      "Iteration 114, loss = 0.80989046\n",
      "Iteration 115, loss = 0.79293422\n",
      "Iteration 116, loss = 0.77596388\n",
      "Iteration 117, loss = 0.75915958\n",
      "Iteration 118, loss = 0.74086761\n",
      "Iteration 119, loss = 0.72371596\n",
      "Iteration 120, loss = 0.70646744\n",
      "Iteration 121, loss = 0.68979233\n",
      "Iteration 122, loss = 0.67316225\n",
      "Iteration 123, loss = 0.65818996\n",
      "Iteration 124, loss = 0.64289796\n",
      "Iteration 125, loss = 0.62778754\n",
      "Iteration 126, loss = 0.61308881\n",
      "Iteration 127, loss = 0.59901295\n",
      "Iteration 128, loss = 0.58528589\n",
      "Iteration 129, loss = 0.57218255\n",
      "Iteration 130, loss = 0.55956648\n",
      "Iteration 131, loss = 0.54737318\n",
      "Iteration 132, loss = 0.53529821\n",
      "Iteration 133, loss = 0.52406752\n",
      "Iteration 134, loss = 0.51310226\n",
      "Iteration 135, loss = 0.50260392\n",
      "Iteration 136, loss = 0.49215946\n",
      "Iteration 137, loss = 0.48235756\n",
      "Iteration 138, loss = 0.47288204\n",
      "Iteration 139, loss = 0.46359812\n",
      "Iteration 140, loss = 0.45458860\n",
      "Iteration 141, loss = 0.44610944\n",
      "Iteration 142, loss = 0.43764684\n",
      "Iteration 143, loss = 0.42942709\n",
      "Iteration 144, loss = 0.42153286\n",
      "Iteration 145, loss = 0.41418309\n",
      "Iteration 146, loss = 0.40674892\n",
      "Iteration 147, loss = 0.39969308\n",
      "Iteration 148, loss = 0.39285103\n",
      "Iteration 149, loss = 0.38604569\n",
      "Iteration 150, loss = 0.37970842\n",
      "Iteration 151, loss = 0.37349367\n",
      "Iteration 152, loss = 0.36741032\n",
      "Iteration 153, loss = 0.36166665\n",
      "Iteration 154, loss = 0.35593938\n",
      "Iteration 155, loss = 0.35043418\n",
      "Iteration 156, loss = 0.34498160\n",
      "Iteration 157, loss = 0.33975216\n",
      "Iteration 158, loss = 0.33476883\n",
      "Iteration 159, loss = 0.32984505\n",
      "Iteration 160, loss = 0.32501425\n",
      "Iteration 161, loss = 0.32038392\n",
      "Iteration 162, loss = 0.31588146\n",
      "Iteration 163, loss = 0.31155334\n",
      "Iteration 164, loss = 0.30708343\n",
      "Iteration 165, loss = 0.30286659\n",
      "Iteration 166, loss = 0.29864016\n",
      "Iteration 167, loss = 0.29469603\n",
      "Iteration 168, loss = 0.29059328\n",
      "Iteration 169, loss = 0.28687869\n",
      "Iteration 170, loss = 0.28307944\n",
      "Iteration 171, loss = 0.27937273\n",
      "Iteration 172, loss = 0.27568916\n",
      "Iteration 173, loss = 0.27200942\n",
      "Iteration 174, loss = 0.26865708\n",
      "Iteration 175, loss = 0.26528904\n",
      "Iteration 176, loss = 0.26187727\n",
      "Iteration 177, loss = 0.25866222\n",
      "Iteration 178, loss = 0.25536221\n",
      "Iteration 179, loss = 0.25239099\n",
      "Iteration 180, loss = 0.24938288\n",
      "Iteration 181, loss = 0.24632044\n",
      "Iteration 182, loss = 0.24345309\n",
      "Iteration 183, loss = 0.24059804\n",
      "Iteration 184, loss = 0.23780469\n",
      "Iteration 185, loss = 0.23511540\n",
      "Iteration 186, loss = 0.23255271\n",
      "Iteration 187, loss = 0.22988423\n",
      "Iteration 188, loss = 0.22732715\n",
      "Iteration 189, loss = 0.22491805\n",
      "Iteration 190, loss = 0.22251881\n",
      "Iteration 191, loss = 0.22010327\n",
      "Iteration 192, loss = 0.21770802\n",
      "Iteration 193, loss = 0.21551639\n",
      "Iteration 194, loss = 0.21317074\n",
      "Iteration 195, loss = 0.21099916\n",
      "Iteration 196, loss = 0.20876946\n",
      "Iteration 197, loss = 0.20672537\n",
      "Iteration 198, loss = 0.20463017\n",
      "Iteration 199, loss = 0.20255538\n",
      "Iteration 200, loss = 0.20044862\n",
      "Iteration 201, loss = 0.19843230\n",
      "Iteration 202, loss = 0.19657677\n",
      "Iteration 203, loss = 0.19458339\n",
      "Iteration 204, loss = 0.19267832\n",
      "Iteration 205, loss = 0.19083149\n",
      "Iteration 206, loss = 0.18901580\n",
      "Iteration 207, loss = 0.18712863\n",
      "Iteration 208, loss = 0.18538771\n",
      "Iteration 209, loss = 0.18357099\n",
      "Iteration 210, loss = 0.18172767\n",
      "Iteration 211, loss = 0.17996432\n",
      "Iteration 212, loss = 0.17822673\n",
      "Iteration 213, loss = 0.17654100\n",
      "Iteration 214, loss = 0.17487468\n",
      "Iteration 215, loss = 0.17315649\n",
      "Iteration 216, loss = 0.17152995\n",
      "Iteration 217, loss = 0.16986463\n",
      "Iteration 218, loss = 0.16826660\n",
      "Iteration 219, loss = 0.16674686\n",
      "Iteration 220, loss = 0.16518104\n",
      "Iteration 221, loss = 0.16367083\n",
      "Iteration 222, loss = 0.16211872\n",
      "Iteration 223, loss = 0.16063231\n",
      "Iteration 224, loss = 0.15916642\n",
      "Iteration 225, loss = 0.15764451\n",
      "Iteration 226, loss = 0.15626960\n",
      "Iteration 227, loss = 0.15488150\n",
      "Iteration 228, loss = 0.15350801\n",
      "Iteration 229, loss = 0.15206060\n",
      "Iteration 230, loss = 0.15075293\n",
      "Iteration 231, loss = 0.14944846\n",
      "Iteration 232, loss = 0.14814027\n",
      "Iteration 233, loss = 0.14688799\n",
      "Iteration 234, loss = 0.14562984\n",
      "Iteration 235, loss = 0.14439389\n",
      "Iteration 236, loss = 0.14319052\n",
      "Iteration 237, loss = 0.14204795\n",
      "Iteration 238, loss = 0.14085642\n",
      "Iteration 239, loss = 0.13966189\n",
      "Iteration 240, loss = 0.13859269\n",
      "Iteration 241, loss = 0.13753019\n",
      "Iteration 242, loss = 0.13634229\n",
      "Iteration 243, loss = 0.13527346\n",
      "Iteration 244, loss = 0.13425416\n",
      "Iteration 245, loss = 0.13320370\n",
      "Iteration 246, loss = 0.13215377\n",
      "Iteration 247, loss = 0.13112037\n",
      "Iteration 248, loss = 0.13011804\n",
      "Iteration 249, loss = 0.12915493\n",
      "Iteration 250, loss = 0.12818651\n",
      "Iteration 251, loss = 0.12726994\n",
      "Iteration 252, loss = 0.12629235\n",
      "Iteration 253, loss = 0.12538690\n",
      "Iteration 254, loss = 0.12448167\n",
      "Iteration 255, loss = 0.12355909\n",
      "Iteration 256, loss = 0.12271132\n",
      "Iteration 257, loss = 0.12181761\n",
      "Iteration 258, loss = 0.12095185\n",
      "Iteration 259, loss = 0.12007404\n",
      "Iteration 260, loss = 0.11921294\n",
      "Iteration 261, loss = 0.11834309\n",
      "Iteration 262, loss = 0.11748348\n",
      "Iteration 263, loss = 0.11664072\n",
      "Iteration 264, loss = 0.11581333\n",
      "Iteration 265, loss = 0.11500155\n",
      "Iteration 266, loss = 0.11419462\n",
      "Iteration 267, loss = 0.11336822\n",
      "Iteration 268, loss = 0.11248244\n",
      "Iteration 269, loss = 0.11171741\n",
      "Iteration 270, loss = 0.11091928\n",
      "Iteration 271, loss = 0.11012083\n",
      "Iteration 272, loss = 0.10930656\n",
      "Iteration 273, loss = 0.10851457\n",
      "Iteration 274, loss = 0.10775823\n",
      "Iteration 275, loss = 0.10700204\n",
      "Iteration 276, loss = 0.10621952\n",
      "Iteration 277, loss = 0.10550404\n",
      "Iteration 278, loss = 0.10474300\n",
      "Iteration 279, loss = 0.10400675\n",
      "Iteration 280, loss = 0.10325394\n",
      "Iteration 281, loss = 0.10252188\n",
      "Iteration 282, loss = 0.10185666\n",
      "Iteration 283, loss = 0.10110141\n",
      "Iteration 284, loss = 0.10042786\n",
      "Iteration 285, loss = 0.09973462\n",
      "Iteration 286, loss = 0.09903458\n",
      "Iteration 287, loss = 0.09836463\n",
      "Iteration 288, loss = 0.09767625\n",
      "Iteration 289, loss = 0.09704036\n",
      "Iteration 290, loss = 0.09637217\n",
      "Iteration 291, loss = 0.09570349\n",
      "Iteration 292, loss = 0.09511485\n",
      "Iteration 293, loss = 0.09447062\n",
      "Iteration 294, loss = 0.09382878\n",
      "Iteration 295, loss = 0.09321384\n",
      "Iteration 296, loss = 0.09257364\n",
      "Iteration 297, loss = 0.09196681\n",
      "Iteration 298, loss = 0.09132410\n",
      "Iteration 299, loss = 0.09071558\n",
      "Iteration 300, loss = 0.09013846\n",
      "Iteration 301, loss = 0.08953690\n",
      "Iteration 302, loss = 0.08894192\n",
      "Iteration 303, loss = 0.08835302\n",
      "Iteration 304, loss = 0.08778036\n",
      "Iteration 305, loss = 0.08720532\n",
      "Iteration 306, loss = 0.08664112\n",
      "Iteration 307, loss = 0.08607044\n",
      "Iteration 308, loss = 0.08551971\n",
      "Iteration 309, loss = 0.08497665\n",
      "Iteration 310, loss = 0.08443429\n",
      "Iteration 311, loss = 0.08391225\n",
      "Iteration 312, loss = 0.08337624\n",
      "Iteration 313, loss = 0.08284437\n",
      "Iteration 314, loss = 0.08230740\n",
      "Iteration 315, loss = 0.08181751\n",
      "Iteration 316, loss = 0.08127909\n",
      "Iteration 317, loss = 0.08081474\n",
      "Iteration 318, loss = 0.08028182\n",
      "Iteration 319, loss = 0.07981661\n",
      "Iteration 320, loss = 0.07931063\n",
      "Iteration 321, loss = 0.07882264\n",
      "Iteration 322, loss = 0.07834700\n",
      "Iteration 323, loss = 0.07787908\n",
      "Iteration 324, loss = 0.07739340\n",
      "Iteration 325, loss = 0.07692604\n",
      "Iteration 326, loss = 0.07648541\n",
      "Iteration 327, loss = 0.07603226\n",
      "Iteration 328, loss = 0.07556650\n",
      "Iteration 329, loss = 0.07511045\n",
      "Iteration 330, loss = 0.07469679\n",
      "Iteration 331, loss = 0.07426050\n",
      "Iteration 332, loss = 0.07381682\n",
      "Iteration 333, loss = 0.07337774\n",
      "Iteration 334, loss = 0.07298560\n",
      "Iteration 335, loss = 0.07254759\n",
      "Iteration 336, loss = 0.07212756\n",
      "Iteration 337, loss = 0.07172608\n",
      "Iteration 338, loss = 0.07131214\n",
      "Iteration 339, loss = 0.07092977\n",
      "Iteration 340, loss = 0.07051168\n",
      "Iteration 341, loss = 0.07013714\n",
      "Iteration 342, loss = 0.06973908\n",
      "Iteration 343, loss = 0.06934462\n",
      "Iteration 344, loss = 0.06894679\n",
      "Iteration 345, loss = 0.06858797\n",
      "Iteration 346, loss = 0.06820278\n",
      "Iteration 347, loss = 0.06783138\n",
      "Iteration 348, loss = 0.06746095\n",
      "Iteration 349, loss = 0.06710196\n",
      "Iteration 350, loss = 0.06671772\n",
      "Iteration 351, loss = 0.06636989\n",
      "Iteration 352, loss = 0.06599309\n",
      "Iteration 353, loss = 0.06565322\n",
      "Iteration 354, loss = 0.06529415\n",
      "Iteration 355, loss = 0.06494259\n",
      "Iteration 356, loss = 0.06458234\n",
      "Iteration 357, loss = 0.06427072\n",
      "Iteration 358, loss = 0.06391684\n",
      "Iteration 359, loss = 0.06357745\n",
      "Iteration 360, loss = 0.06325137\n",
      "Iteration 361, loss = 0.06290428\n",
      "Iteration 362, loss = 0.06257022\n",
      "Iteration 363, loss = 0.06224259\n",
      "Iteration 364, loss = 0.06190658\n",
      "Iteration 365, loss = 0.06159331\n",
      "Iteration 366, loss = 0.06127489\n",
      "Iteration 367, loss = 0.06092305\n",
      "Iteration 368, loss = 0.06060713\n",
      "Iteration 369, loss = 0.06027614\n",
      "Iteration 370, loss = 0.05996886\n",
      "Iteration 371, loss = 0.05964395\n",
      "Iteration 372, loss = 0.05933276\n",
      "Iteration 373, loss = 0.05902028\n",
      "Iteration 374, loss = 0.05870317\n",
      "Iteration 375, loss = 0.05840170\n",
      "Iteration 376, loss = 0.05808184\n",
      "Iteration 377, loss = 0.05778233\n",
      "Iteration 378, loss = 0.05748526\n",
      "Iteration 379, loss = 0.05718121\n",
      "Iteration 380, loss = 0.05687380\n",
      "Iteration 381, loss = 0.05658266\n",
      "Iteration 382, loss = 0.05629929\n",
      "Iteration 383, loss = 0.05599808\n",
      "Iteration 384, loss = 0.05571892\n",
      "Iteration 385, loss = 0.05542932\n",
      "Iteration 386, loss = 0.05513418\n",
      "Iteration 387, loss = 0.05486111\n",
      "Iteration 388, loss = 0.05459053\n",
      "Iteration 389, loss = 0.05431295\n",
      "Iteration 390, loss = 0.05406034\n",
      "Iteration 391, loss = 0.05376842\n",
      "Iteration 392, loss = 0.05349103\n",
      "Iteration 393, loss = 0.05323541\n",
      "Iteration 394, loss = 0.05296360\n",
      "Iteration 395, loss = 0.05271724\n",
      "Iteration 396, loss = 0.05243804\n",
      "Iteration 397, loss = 0.05219045\n",
      "Iteration 398, loss = 0.05191419\n",
      "Iteration 399, loss = 0.05166777\n",
      "Iteration 400, loss = 0.05140956\n",
      "Iteration 401, loss = 0.05114380\n",
      "Iteration 402, loss = 0.05090775\n",
      "Iteration 403, loss = 0.05065585\n",
      "Iteration 404, loss = 0.05040497\n",
      "Iteration 405, loss = 0.05017389\n",
      "Iteration 406, loss = 0.04992248\n",
      "Iteration 407, loss = 0.04966843\n",
      "Iteration 408, loss = 0.04944016\n",
      "Iteration 409, loss = 0.04921259\n",
      "Iteration 410, loss = 0.04898242\n",
      "Iteration 411, loss = 0.04874355\n",
      "Iteration 412, loss = 0.04850999\n",
      "Iteration 413, loss = 0.04828761\n",
      "Iteration 414, loss = 0.04806318\n",
      "Iteration 415, loss = 0.04783610\n",
      "Iteration 416, loss = 0.04761600\n",
      "Iteration 417, loss = 0.04738882\n",
      "Iteration 418, loss = 0.04719353\n",
      "Iteration 419, loss = 0.04694907\n",
      "Iteration 420, loss = 0.04673503\n",
      "Iteration 421, loss = 0.04653619\n",
      "Iteration 422, loss = 0.04633362\n",
      "Iteration 423, loss = 0.04611592\n",
      "Iteration 424, loss = 0.04589871\n",
      "Iteration 425, loss = 0.04569691\n",
      "Iteration 426, loss = 0.04548277\n",
      "Iteration 427, loss = 0.04526276\n",
      "Iteration 428, loss = 0.04509160\n",
      "Iteration 429, loss = 0.04487081\n",
      "Iteration 430, loss = 0.04467880\n",
      "Iteration 431, loss = 0.04445536\n",
      "Iteration 432, loss = 0.04426865\n",
      "Iteration 433, loss = 0.04408021\n",
      "Iteration 434, loss = 0.04388052\n",
      "Iteration 435, loss = 0.04367891\n",
      "Iteration 436, loss = 0.04349786\n",
      "Iteration 437, loss = 0.04329362\n",
      "Iteration 438, loss = 0.04312329\n",
      "Iteration 439, loss = 0.04292713\n",
      "Iteration 440, loss = 0.04271715\n",
      "Iteration 441, loss = 0.04255765\n",
      "Iteration 442, loss = 0.04236645\n",
      "Iteration 443, loss = 0.04217483\n",
      "Iteration 444, loss = 0.04200243\n",
      "Iteration 445, loss = 0.04182142\n",
      "Iteration 446, loss = 0.04163715\n",
      "Iteration 447, loss = 0.04146945\n",
      "Iteration 448, loss = 0.04128765\n",
      "Iteration 449, loss = 0.04111142\n",
      "Iteration 450, loss = 0.04093292\n",
      "Iteration 451, loss = 0.04076637\n",
      "Iteration 452, loss = 0.04059958\n",
      "Iteration 453, loss = 0.04041886\n",
      "Iteration 454, loss = 0.04025576\n",
      "Iteration 455, loss = 0.04009418\n",
      "Iteration 456, loss = 0.03993190\n",
      "Iteration 457, loss = 0.03974310\n",
      "Iteration 458, loss = 0.03959768\n",
      "Iteration 459, loss = 0.03943650\n",
      "Iteration 460, loss = 0.03927530\n",
      "Iteration 461, loss = 0.03911119\n",
      "Iteration 462, loss = 0.03895199\n",
      "Iteration 463, loss = 0.03878853\n",
      "Iteration 464, loss = 0.03863453\n",
      "Iteration 465, loss = 0.03847493\n",
      "Iteration 466, loss = 0.03832227\n",
      "Iteration 467, loss = 0.03816225\n",
      "Iteration 468, loss = 0.03801380\n",
      "Iteration 469, loss = 0.03785898\n",
      "Iteration 470, loss = 0.03769721\n",
      "Iteration 471, loss = 0.03755666\n",
      "Iteration 472, loss = 0.03740409\n",
      "Iteration 473, loss = 0.03725611\n",
      "Iteration 474, loss = 0.03711876\n",
      "Iteration 475, loss = 0.03695786\n",
      "Iteration 476, loss = 0.03681564\n",
      "Iteration 477, loss = 0.03666792\n",
      "Iteration 478, loss = 0.03651727\n",
      "Iteration 479, loss = 0.03637911\n",
      "Iteration 480, loss = 0.03622936\n",
      "Iteration 481, loss = 0.03608888\n",
      "Iteration 482, loss = 0.03594737\n",
      "Iteration 483, loss = 0.03579726\n",
      "Iteration 484, loss = 0.03566773\n",
      "Iteration 485, loss = 0.03552691\n",
      "Iteration 486, loss = 0.03538792\n",
      "Iteration 487, loss = 0.03525193\n",
      "Iteration 488, loss = 0.03512736\n",
      "Iteration 489, loss = 0.03497848\n",
      "Iteration 490, loss = 0.03485580\n",
      "Iteration 491, loss = 0.03471073\n",
      "Iteration 492, loss = 0.03458735\n",
      "Iteration 493, loss = 0.03445040\n",
      "Iteration 494, loss = 0.03431450\n",
      "Iteration 495, loss = 0.03418195\n",
      "Iteration 496, loss = 0.03406021\n",
      "Iteration 497, loss = 0.03392861\n",
      "Iteration 498, loss = 0.03380534\n",
      "Iteration 499, loss = 0.03366580\n",
      "Iteration 500, loss = 0.03353650\n",
      "Iteration 501, loss = 0.03343180\n",
      "Iteration 502, loss = 0.03329158\n",
      "Iteration 503, loss = 0.03317466\n",
      "Iteration 504, loss = 0.03304325\n",
      "Iteration 505, loss = 0.03292022\n",
      "Iteration 506, loss = 0.03280980\n",
      "Iteration 507, loss = 0.03268668\n",
      "Iteration 508, loss = 0.03256597\n",
      "Iteration 509, loss = 0.03245179\n",
      "Iteration 510, loss = 0.03233718\n",
      "Iteration 511, loss = 0.03221524\n",
      "Iteration 512, loss = 0.03209813\n",
      "Iteration 513, loss = 0.03198525\n",
      "Iteration 514, loss = 0.03186510\n",
      "Iteration 515, loss = 0.03175402\n",
      "Iteration 516, loss = 0.03164519\n",
      "Iteration 517, loss = 0.03151784\n",
      "Iteration 518, loss = 0.03141851\n",
      "Iteration 519, loss = 0.03130180\n",
      "Iteration 520, loss = 0.03118067\n",
      "Iteration 521, loss = 0.03106584\n",
      "Iteration 522, loss = 0.03096320\n",
      "Iteration 523, loss = 0.03084844\n",
      "Iteration 524, loss = 0.03073334\n",
      "Iteration 525, loss = 0.03062316\n",
      "Iteration 526, loss = 0.03051472\n",
      "Iteration 527, loss = 0.03040023\n",
      "Iteration 528, loss = 0.03030036\n",
      "Iteration 529, loss = 0.03018343\n",
      "Iteration 530, loss = 0.03008906\n",
      "Iteration 531, loss = 0.02996991\n",
      "Iteration 532, loss = 0.02986895\n",
      "Iteration 533, loss = 0.02976278\n",
      "Iteration 534, loss = 0.02965509\n",
      "Iteration 535, loss = 0.02955150\n",
      "Iteration 536, loss = 0.02945267\n",
      "Iteration 537, loss = 0.02934515\n",
      "Iteration 538, loss = 0.02924041\n",
      "Iteration 539, loss = 0.02914099\n",
      "Iteration 540, loss = 0.02904151\n",
      "Iteration 541, loss = 0.02893417\n",
      "Iteration 542, loss = 0.02883984\n",
      "Iteration 543, loss = 0.02874305\n",
      "Iteration 544, loss = 0.02863329\n",
      "Iteration 545, loss = 0.02853364\n",
      "Iteration 546, loss = 0.02843698\n",
      "Iteration 547, loss = 0.02833241\n",
      "Iteration 548, loss = 0.02824046\n",
      "Iteration 549, loss = 0.02813629\n",
      "Iteration 550, loss = 0.02804223\n",
      "Iteration 551, loss = 0.02793873\n",
      "Iteration 552, loss = 0.02785090\n",
      "Iteration 553, loss = 0.02775092\n",
      "Iteration 554, loss = 0.02765944\n",
      "Iteration 555, loss = 0.02755723\n",
      "Iteration 556, loss = 0.02745777\n",
      "Iteration 557, loss = 0.02737017\n",
      "Iteration 558, loss = 0.02727643\n",
      "Iteration 559, loss = 0.02718743\n",
      "Iteration 560, loss = 0.02709197\n",
      "Iteration 561, loss = 0.02699621\n",
      "Iteration 562, loss = 0.02690313\n",
      "Iteration 563, loss = 0.02682469\n",
      "Iteration 564, loss = 0.02672746\n",
      "Iteration 565, loss = 0.02663337\n",
      "Iteration 566, loss = 0.02654103\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Model Weights:\n",
      "[(8, 10), (10, 10), (10, 9)]\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(random_state=1, hidden_layer_sizes=(10,10),max_iter=1000, verbose=True).fit(X_train_lda, y_train)\n",
    "print(\"Model Weights:\")\n",
    "model_info = [coef.shape for coef in clf.coefs_]\n",
    "print(model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66feb398-2cfe-4380-9f37-36b89069caae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
